<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[CarrollOps]]></title>
  <link href="http://www.carrollops.com/atom.xml" rel="self"/>
  <link href="http://www.carrollops.com/"/>
  <updated>2012-08-10T16:47:57-07:00</updated>
  <id>http://www.carrollops.com/</id>
  <author>
    <name><![CDATA[Jeremy Carroll]]></name>
    <email><![CDATA[phobos182 gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop Fair Scheduler Monitoring - Part 2]]></title>
    <link href="http://www.carrollops.com/blog/2012/08/10/hadoop-fair-scheduler-monitoring-part-2/"/>
    <updated>2012-08-10T08:44:00-07:00</updated>
    <id>http://www.carrollops.com/blog/2012/08/10/hadoop-fair-scheduler-monitoring-part-2</id>
    <content type="html"><![CDATA[<p>In my first blog post I went over setting up a simple fair scheduler monitoring script using mechanize to scrape stats from the JobTracker page. Now that all the metrics are in our graphite system (In this case, I used graphite due to nice front-end vis) I wanted to show how we can put these metrics to work.</p>

<p>The goal was to measure requests for resources (Map slots / Reduce slots) for different job queues. I can then determine certain parts of the day where there is a lot of contention for resources to see if a job can be rescheduled to a different time when there is more slots available. Another option would be to change the fair-scheduler minShare, preemption, and weight settings to help shape the SLA of running jobs. Before we can start setting these policies it would be nice to know map / reduce slot demand over a timeline. Also look at &#8216;running&#8217; map / reduce slots to see what resources the scheduler gave during those time periods. Here are some questions the metrics can answer for us.</p>

<ul>
<li>During what times of the day is the cluster heavily over-subscribed?

<ul>
<li>Is the cluster using more map or reduce slots during these timeframes?</li>
</ul>
</li>
<li>What queues / users are requesting resources?

<ul>
<li>How many jobs are they running?</li>
<li>How many Map / Reduce slots are they requesting?</li>
<li>How fast are they completing map / reduce slots?</li>
</ul>
</li>
</ul>


<p>I put together a series of visualizations to help debug an oversubscribed Hadoop cluster. Using this information was invaluable in determining winners and losers of cluster resources. It did not help debug individual MapReduce jobs. You would have to look at individual job performance such as spilling to disk, not using a combiner or secondary sort, or sending too much data uncompressed over the wire, etc&#8230; But it does help determine overall cluster utilization.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_demand.png" class="fancybox fancybox-a" title="Demand "><img src="http://www.carrollops.com/images/fair_sched_2_demand_m.png" alt="Demand " /></a> The image here represents map + reduce slots requested by running jobs over a 24 hour period. As jobs complete, the total number drops. When you start to see solid straight lines (plataus) in the image, it means that no new jobs have been added or removed from the scheduler for this pool. The current jobs are just taking a while to complete. Nice to get a big picture of how many tasks are being run by the cluster by pool.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_jobs.png" class="fancybox fancybox-a" title="Jobs "><img src="http://www.carrollops.com/images/fair_sched_2_jobs_m.png" alt="Jobs " /></a> This graph shows how many jobs are currently in the fair scheduler by pool. Easy to see bursts of activity from end users. The primary purpose of this is to help tune the maxRunningJobs parameters per queue. It&#8217;s also nice to see day / night patterns of activity during for processing cycles. See if somebody can shift a job a couple hours to take advantage of lower utilization periods.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_map_slots.png" class="fancybox fancybox-a" title="Map Slots "><img src="http://www.carrollops.com/images/fair_sched_2_map_slots_m.png" alt="Map Slots " /></a> This metric shows the amount of map slots running at any given time. It&#8217;s great to determine what pools are taking the majority of the resources. Also shows if there are long running jobs taking the majority of the queue for long periods of time. With preemption policies enabled, you can also see slots getting killed to free up space for a pool to meet it&#8217;s minShare / fairShare.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_maps_speed.png" class="fancybox fancybox-a" title="Map Speed "><img src="http://www.carrollops.com/images/fair_sched_2_maps_speed_m.png" alt="Map Speed " /></a> This is one of my favorite graphs from this tool. You can see the total number of map slots requested versus the number of map tasks completed. Basically shows you job progress at a high level, and how fast the tasks are completing. Since this does not take into account new jobs entering and adding tasks, it&#8217;s a good high level gut check.
<a href="http://www.carrollops.com/images/fair_sched_2_maps_completed.png" class="fancybox fancybox-a" title="Maps Outstanding "><img src="http://www.carrollops.com/images/fair_sched_2_maps_completed_m.png" alt="Maps Outstanding " /></a> Using Graphite&#8217;s function for a difference of two series, this is the same graph as the image before, but by dividing the total map slots by maps completed. This will give you the number of remaining map tasks to be processed.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_queue.png" class="fancybox fancybox-a" title="Queue "><img src="http://www.carrollops.com/images/fair_sched_2_queue_m.png" alt="Queue " /></a> As with most of our jobs the map side completes quickly, but blocks for long periods of time waiting for data to be shuffled across the network. So we usually have contention on reduce slots, not map slots. Using some of graphite draw in second Y axis features I can look at the number of jobs in a queue versus the number of reduces (completed + total). Flat lines lets me know a long running reduce task, or a lot of intermediate output.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_reduce_slots.png" class="fancybox fancybox-a" title="Reduce Slots "><img src="http://www.carrollops.com/images/fair_sched_2_reduce_slots_m.png" alt="Reduce Slots " /></a> I currently have a minShare policy on a queue that has a strict SLA. It was nice to see that the share was being met as shown by the blue line drawn. This queue always gets its slots via preemption. You can also see we have long periods of contention as 100% of the reduce slots are taken for hours at a time. Using this graph I am currently tuning the cluster to give up map slots in favor of reduce slots in addition to increasing the block size as map tasks are completing very quickly.</p>

<p><a href="http://www.carrollops.com/images/fair_sched_2_queue_info.png" class="fancybox fancybox-a" title="Queue Info "><img src="http://www.carrollops.com/images/fair_sched_2_queue_info_m.png" alt="Queue Info " /></a> This is the same graph as the map side by dividing the total number of reduce slots requested by reduces completed. Cross referencing this with our other network graphs shows a lot of intermediate data going across the network.</p>

<p>All in all I have had good success with a very simple metrics collection tool to help me identify some bottlenecks on my cluster setup. This is what I have come up with so far.</p>

<ul>
<li>Increase the number of reduce slots.</li>
<li>Increase the amount of work done by a map task as they are completing very quickly.</li>
<li>Shuffle some job start times to take advantage of idle resources.</li>
</ul>


<p>Let me know if anybody has a chance to use the tool to identify issues with their cluster. I&#8217;d love to hear some feedback.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Fair Scheduler monitoring]]></title>
    <link href="http://www.carrollops.com/blog/2012/08/07/hadoop-fair-scheduler-monitoring/"/>
    <updated>2012-08-07T15:23:00-07:00</updated>
    <id>http://www.carrollops.com/blog/2012/08/07/hadoop-fair-scheduler-monitoring</id>
    <content type="html"><![CDATA[<p>So today I received a request to help debug fair scheduler performance on one of our large Hadoop clusters. Normally this is the point where I would point to installing a tool like Cloudera Manager, but we do not have CM running anywhere within our environment. So I took a quick look around GitHub to see if anybody has written any scripts to monitor the fair scheduler allocations and found nothing. We currently have monitoring on the Hadoop / HDFS level, but are lacking visibility at the individual scheduler / pool level. Questions arise such as this that I cannot answer without a cool made-up story.</p>

<ul>
<li>My job ran very slow last night, can you take a look at the cluster to see if anything is wrong?</li>
<li>This set of Oozie jobs normally takes 2 hours, but over the past few days it has been taking 4 hours easy run, why is that?</li>
<li>Can you check the network? Looks like my jobs have been running very slowly.</li>
</ul>


<p>Now at this point, I check the regular cluster health with our fleet of monitoring tools based on Graphite / OpenTSDB. Looking at HDFS health, I see no instances of failed datanodes, 100 mbit links, errors in the log files, et al. While looking at basic hadoop-metrics from our logging context, I see that the cluster is almost always 100% utilized on map slots / reduce slots. The next obvious question is &#8216;Who is running jobs stealing my resources?&#8217;. Before I can start to create fair-share policies, and pick winners and losers of precious cluster resources via preemption I need to know demand. I need to know how many jobs are running in each pool, and how many slots they require to finish there tasks. I would also love to monitor preemption requests to see when pools start killing other tasks to meet their fair-share or min-share. I set out to create a simple tool to query the http://jobtracker:50030/scheduler?advanced web interface on a timer, and send this metadata to Graphite / OpenTSDB on a real-time basis. I could then create visualizations to see demand and allocation to help craft fair share policies. It&#8217;s not perfect as it does not look at individual job performance (Input splits, min / max task completion time) but is really helpful on a high level.</p>

<p>I wanted something quick and easy to get done, so I took about 2 hours to create a Ruby Mechanize script to screen scrape the jobtracker fair-scheduler page. I then turn the output into a KeyValue format that I can use with OpenTSDB / Graphite. I did not want to create a lot of unique keys due to issues with Graphite creating a Whisper database per unique point. So capturing job-name, task-id is unacceptable. I instead aggregate metrics by user, or pool. In the case of our cluster job pool names are users. So I aggreagte the metrics by pool name, then send off to our visualiation system for further planning.</p>

<ul>
<li><a href="https://github.com/phobos182/hadoop-hbase-tools">https://github.com/phobos182/hadoop-hbase-tools</a></li>
</ul>


<p>I created a project above if you would like to hack on the code. Currently I am using Diamond <a href="https://github.com/BrightcoveOS/Diamond">https://github.com/BrightcoveOS/Diamond</a> to schedule checks via the UserScriptsCollector for ruby programs. It wants Key + Value, and fills in the date for you based on your scheduler. I can then send the metrics to both OpenTSDB + Graphite with the same system via Handlers. Below is some graphs of the things you can do with this information.</p>

<p><img class="left" src="http://www.carrollops.com/images/graphite_fair_scheduler.png" width="351" height="140" title="'Fair scheduler' #2" >
As you can see by the graph, it details a nice break down of slots utilization by fair scheduler pool. In this image, 3 different pools are racing for resources as the cluster is 100% utilized. You can dive into other metrics such as total tasks scheduled (map / reduce) vs. resources available at that time. Let me know if you find this tool helpful. Pull requests are always welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open sourcing some work]]></title>
    <link href="http://www.carrollops.com/blog/2012/08/06/open-sourcing-some-work/"/>
    <updated>2012-08-06T12:37:00-07:00</updated>
    <id>http://www.carrollops.com/blog/2012/08/06/open-sourcing-some-work</id>
    <content type="html"><![CDATA[<p>I&#8217;ve made it a point to start contributing more of my work back to the open source community i&#8217;ve gained so much from. So in next few days with permission from my employer, I am going to start contributing some simple tools that have made my life easier over here at Klout. Here is a list of some of the things i&#8217;m working on.</p>

<ul>
<li>Storm Puppet module</li>
<li>Storm debian packaging</li>
<li>HBase rolling compact tool with ZooKeeper locks</li>
<li>HBase major_compaction script with ZooKeeper locks</li>
<li>HBase metrics collection script</li>
<li>HBase per-table load balancing calculation script</li>
<li>Hadoop GraphiteClient context</li>
<li>Kafka JMX Monitoring script</li>
<li>Diamond handler for Kafka</li>
<li>OpenTSDB GnuPlot options</li>
<li>MCollective Monit agent</li>
<li>Hadoop hiera puppet module</li>
<li>HBase hiera puppet module</li>
</ul>

]]></content>
  </entry>
  
</feed>
